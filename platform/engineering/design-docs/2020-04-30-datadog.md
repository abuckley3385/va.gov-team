# DataDog Design Doc

_Replace the previous line with the the title of your project or component and replace the following lines with your name(s), email and the date._  
**Author(s):** 
**Last Updated:** April 30, 2020  
**Status:** **Draft** | In Review | Approved  
**Approvers:** _Person A_ \[ \], _Person B_ \[ \], ...  
_Replace the previous line with 2-4 people, all of whom must explicitly approve your design proposal. An important part of the design doc is building consensus with key stakeholders, technical implementers who you'll work with, and technical contacts for other systems that this proposal affects or is affected by._

_A design document is not a press release, a vision statement, a research report, or a business plan._

_The intended audience for this document is software engineers, but it should make sense to anyone familiar with software development._


## Overview

### Objective

This doc is meant to provide a place to collaborate around a proposal to replace the current Prometheus + Grafana monitoring stack on VSP with DataDog and make a decision with buy-in across the VSP program. Because of the nature of the decision, they may read as a product pitch, but we want to make sure we capture all the ways that it compares to the current solution and what additional features may be useful.

We are writing this assuming that some other technical components around the Prometheus system do not change immediately. What's not on the table here are:

- AWS (GovCloud) as a hosting environment
- PagerDuty
- Nomad/Consul/Vault stack
- Slack
- vets-api does not get split into many services, but we should consider a future where that could be a reality

### Background

The program is at a crossroad with respect to monitoring. We are in the process of overhauling the way that we manage infrastructure and deployments, so the current monitoring solution needs to be adapted. The current installation of Prometheus is also very out-of-date and likely needs to be rebuilt from scratch in order to move forward. Some members of the team have knowledge of Prometheus, but it's currently a small number. So, we are at a place where we need to choose whether we have a lot of new team members on ramping into Prometheus knowledge for a new implementation or decide to pursue another option.

There are a lot of options on the market today. A key decision included here is SaaS vs self-hosted. Both have advantages and disadvantages. The solution being proposed here is using DataDog which is SaaS with an agent installed on each node within the infrastructure. DataDog also has a mixed push and poll method, whereas Prometheus is strictly poll.

### High Level Design

The proposal here is to utilize DataDog to replace the current Prometheus and Grafana implementations. DataDog would be the place that we would collect and store metrics, build dashboards for presenting that information, and send alerts out to on-call engineers through PagerDuty and Slack.

<insert diagram of before/after>

To do this, there's some features we'd like to have access to:

- Infrastructure
- APM
- Synthetic monitors
- AWS integration

Features that we like to utilize but believe are not sure will be feasible compliance-wise:

- Logging
- Real User Monitoring

The infrastructure feature replaces the majority of the exporters we use for Prometheus. It will gather things like CPU/memory/disk statistics, information about containers (and their CPU/memory/disk statistics), and enable us to gather metrics from other things like HAProxy or custom scripts. The agent also listens locally for StatsD metrics which we can use for everything that's currently sending to StatsD (vets-api, Sentry).

APM is also available once the agent is installed. Additional tracing code has to be added to vets-api to get the full APM experience. This would be a much-needed feature that is currently not available in our monitoring stack.

Synthetic monitors replace the Prometheus Blackbox Exporters and Route53 Health Checks in place today, and are the way that we would monitor endpoints from across the public internet.

Enabling AWS integrations gets events about changes from AWS to be integrated alongside metrics from the systems. It enables new alerting and contextualization in dashboards that is not available today.

## Specifics

TODO meta: shifted voice here to be more design doc about the solution rather than talking about its capabilities, oops. Adjust above here.

### Detailed Design

DataDog is an agent-based monitoring system. An agent would be installed onto each node in the infrastructure and the agent has the capability to poll for data via "integrations" or receive data via DogStatsD. The agent then sends that data to the DataDog service where it can be queried via API and visualized in various ways.

#### Sending metrics

DogStatsD is an extension of the StatsD protocol with added tagging capabilities which seems to be the primary method that DataDog wants to have metrics sent to it. Poll integrations would be avoided where possible and done only for the lower level infrastructure components like Nomad, Consul, Vault, Docker, etc.

Today, vets-api sends most metrics with tags via the DogStatsD protocol. Luckily, we should be able to just change the destination for these metrics and the change for metrics that aren't using tags yet would be minimal. The [Prometheus StatsD exporter has mapping](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/deployment/config/vets-api/statsd-exporter-mapping.conf) for all of the metrics it receives which would need to be adjusted. We can expose this as a service for VFS products to send metrics in and have them stored without any additional configuration by the VSP team.

The agent also provides a listener for APM data, it's a local HTTP server. Tracing code has to be added for most APM information. Ruby is supported with a full guide in the [documentation](https://docs.datadoghq.com/tracing/setup/ruby/). The product uses [OpenTracing](https://docs.datadoghq.com/tracing/opentracing/), so we can swap providers in the future should we need to for whatever reason.

#### Using metrics

The biggest area that we've invested in Prometheus + Grafana to date has been around alerting and dashboards. All of the basic constructs within DataDog will be managed by Terraform to manage changes.

TODO Open question: Would we want to have multiple accounts? Could we segregate by environment (i.e. production account, staging account, etc) or team (i.e. GIDS account, CMS account, etc)? We also don't know what user management looks like

Similar to Grafana, we can build dashboards with varying visualization types built in, and we can use tags for template variables to create generalized dashboards for VFS teams to filter down to only apps or services they care about.

Alerts are called "monitors" in DataDog. The VSP team would setup some default monitors for an application (% of 500 errors for a given app) similar to the way that Prometheus is setup today, and allow VFS teams to define their own rules for custom metrics that are sent via DogStatsD.

### Code Location
_The path of the source code in the repository._

### Testing Plan
_How you will verify the behavior of your system. Once the system is written, this section should be updated to reflect the current state of testing and future aspirations._

### Logging
_What your system will record and how._

### Debugging
_How users can debug interactions with your system. When designing a system it's important to think about what tools you can provide to make debugging problems easier. Sometimes it's unclear whether the problem is in your system at all, so a mechanism for isolating a particular interaction and examining it to see if your system behaved as expected is very valuable. Once a system is in use, this is a great place to put tips and recipes for debugging. If this section grows too large, the mechanisms can be summarized here and individual tips can be moved to another document._

### Caveats
_Gotchas, differences between the design and implementation, other potential stumbling blocks for users or maintainers, and their implications and workarounds. Unless something is known to be tricky ahead of time, this section will probably start out empty._

_Rather than deleting it, it's recommended that you keep this section with a simple place holder, since caveats will almost certainly appear down the road._

_To be determined._

### Security Concerns
_This section should describe possible threats (denial of service, malicious requests, etc) and what, if anything, is being done to protect against them. Be sure to list concerns for which you don't have a solution or you believe don't need a solution. Security concerns that we don't need to worry about also belong here (e.g. we don't need to worry about denial of service attacks for this system because it only receives requests from the api server which already has DOS attack protections)._

### Privacy Concerns
_This section should describe any risks related to user data, PII that are added by this new application. Think about flows of user data through systems, places data is stored and logged, places data is displayed to users. Where is user data stored or logged? How long is it stored?_

### Open Questions and Risks
_This section should describe design questions that have not been decided yet, research that needs to be done and potential risks that could make make this system less effective or more difficult to implement._

_Some examples are: Should we communicate using TCP or UDP? How often do we expect our users to interrupt running jobs? This relies on an undocumented third-party API which may be turned off at any point._

_For each question you should include any relevant information you know. For risks you should include estimates of likelihood, cost if they occur and ideas for possible workarounds._

### Work Estimates
_Split the work into milestones that can be delivered, put them in the order that you think they should be done, and estimate roughly how much time you expect it each milestone to take. Ideally each milestone will take one week or less._

### Alternatives

There's a few alternatives to this. We have a doc describing decision points for the Ops team in the [infrastructure recommit project folder](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/platform/infrastructure_recommit/monitoring/solution_evaluation.md). Some brief details on decision points below.

#### Re-implement using Prometheus

We could re-implement monitoring using Prometheus. We have some in-house knowledge already, but it's a limited set of individuals. In general, the biggest issues with Prometheus have been that its query language has been hard for a lot of engineers and we have to be very careful about the performance with cardinality. The current implementation has been allowed to fall way out-of-date, which we could address with some re-commitment on that front, but it's also a burden for us to keep it updated.

#### Explore other solutions

Most other open source / self-hosted solutions need a large number of components put together. While they are technically capable, our time could be better spent in other areas of the platform outside of re-learning and building a new solution.

There are other SaaS providers as well, but only NewRelic and Dynatrace are known to have hope of becoming approved for use anytime soon. NewRelic has fantastic APM functionality, but is weak on the infrastructure side. Dynatrace doesn't have Ruby support which is a non-starter given that the two main APIs we operate are Ruby on Rails applications.

### Future Work
_Features you'd like to (or will need to) add but aren't required for the current release. This is a great place to speculate on potential features and performance improvements._

### Revision History
_The table below should record the major changes to this document. You don't need to add an entry for typo fixes, other small changes or changes before finishing the initial draft._

Date | Revisions Made | Author
-----|----------------|--------
April 30, 2020 | Initial draft | Wyatt Walter
